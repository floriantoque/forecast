{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF INVERTED - multi/uni -> UNI (one model per time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../utils/')\n",
    "from utils import *\n",
    "from pylab import *\n",
    "from utils_date import *\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_xy_dataset(df_Xy, time_series, features_exogenous, features_context):\n",
    "    df_Xy = copy.deepcopy(df_Xy[time_series+features_exogenous+features_context].dropna())\n",
    "    days = sorted(list(set([i[:10] for i in df_Xy.index.values])))\n",
    "    Xnames = [f+'-T'+str(ix)for f in features_exogenous for ix in np.arange(96)] + features_context\n",
    "    \n",
    "    X = []\n",
    "    list_y=[]\n",
    "    for d in tqdm(days,desc='Days loop'):\n",
    "        ex = df_Xy.loc[d+' 00:00:00': d+ ' 23:45:00'][features_exogenous].values.T.flatten()\n",
    "        co = df_Xy.loc[[d+' 00:00:00']][features_context].values.flatten()\n",
    "        X.append(np.concatenate([ex, co]))\n",
    "        y = []\n",
    "        for s in time_series:\n",
    "            y.append(df_Xy.loc[d+' 00:00:00': d+ ' 23:45:00'][s].values)\n",
    "        list_y.append(y)\n",
    "        \n",
    "    return np.array(X), np.swapaxes(np.array(list_y),0,1), Xnames, days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AT=5\n",
    "def mse(obs, pred):\n",
    "    return ((pred - obs) ** 2).mean()\n",
    "\n",
    "def rmse(obs, pred):\n",
    "    return np.sqrt(mse(obs, pred))\n",
    "\n",
    "def mae(obs, pred):\n",
    "    return np.absolute(pred - obs).mean()\n",
    "\n",
    "def mape_at(obs, pred):\n",
    "    mask = obs >= AT\n",
    "    return ((np.absolute(pred[mask] - obs[mask]) / obs[mask]).mean())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_predict(X, y, cv, param_grid, scaler_choice_X, scaler_choice_y):\n",
    "    \n",
    "    pred_train_array = []\n",
    "    pred_val_array = []\n",
    "    ytrain_array = []\n",
    "    yval_array = []\n",
    "    \n",
    "    for ix_train,ix_val in cv.split(X):\n",
    "        Xtrain,Xval = X[ix_train], X[ix_val]\n",
    "        ytrain,yval = y[ix_train], y[ix_val]\n",
    "\n",
    "        scalerX = None\n",
    "        scalery = None\n",
    "        if scaler_choice_X == 'minmax':\n",
    "            scalerX = MinMaxScaler(feature_range=(0, 1))\n",
    "        elif scaler_choice_X == 'standard':\n",
    "            scalerX = StandardScaler()\n",
    "        if scalerX != None:\n",
    "            Xtrain = scalerX.fit_transform(Xtrain)\n",
    "            Xval = scalerX.transform(Xval)\n",
    "\n",
    "        if scaler_choice_y == 'minmax':\n",
    "            scalery = MinMaxScaler(feature_range=(0, 1))\n",
    "        elif scaler_choice_y == 'standard':\n",
    "            scalery = StandardScaler()\n",
    "        if scalery != None:\n",
    "            ytrain = scalery.fit_transform(ytrain)\n",
    "            yval = scalery.transform(yval)\n",
    "\n",
    "        keys, values = zip(*param_grid.items())\n",
    "        all_params = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "        pred_train_params = []\n",
    "        pred_val_params = []\n",
    "        for p in all_params:\n",
    "            rf = RandomForestRegressor(**p, verbose=0)\n",
    "            rf.fit(Xtrain,ytrain)\n",
    "            pred_train = rf.predict(Xtrain)\n",
    "            pred_val = rf.predict(Xval)\n",
    "            pred_train_params.append(pred_train)\n",
    "            pred_val_params.append(pred_val)\n",
    "\n",
    "        pred_train_array.append(pred_train_params)\n",
    "        pred_val_array.append(pred_val_params)\n",
    "        ytrain_array.append(ytrain)\n",
    "        yval_array.append(yval)\n",
    "\n",
    "    pred_train_array = np.array(pred_train_array)\n",
    "    pred_val_array = np.array(pred_val_array)\n",
    "    ytrain_array = np.array(ytrain_array)\n",
    "    yval_array = np.array(yval_array) \n",
    "    \n",
    "    return pred_train_array, pred_val_array, ytrain_array, yval_array\n",
    "\n",
    "\n",
    "def optimize(X, y_list, param_kfold, time_series):\n",
    "\n",
    "    cv = KFold(**param_kfold)\n",
    "    \n",
    "    # loop over y_list\n",
    "    pred_train_all = []\n",
    "    pred_val_all = []\n",
    "    ytrain_all = []\n",
    "    yval_all = []\n",
    "\n",
    "    for y, ts in tqdm(zip(y_list,time_series),desc='Optimization time_series'):\n",
    "        pred_train_array, pred_val_array, ytrain_array, yval_array = fit_predict(X, y, cv, param_grid, scaler_choice_X, scaler_choice_y)\n",
    "        pred_train_all.append(pred_train_array)\n",
    "        pred_val_all.append(pred_val_array)\n",
    "        ytrain_all.append(ytrain_array)\n",
    "        yval_all.append(yval_array)\n",
    "\n",
    "    pred_train_all = np.array(pred_train_all)\n",
    "    pred_val_all = np.array(pred_val_all)\n",
    "\n",
    "    ytrain_all = np.array(ytrain_all)\n",
    "    yval_all = np.array(yval_all)\n",
    "\n",
    "    # Errors calculus\n",
    "    pred_train = np.swapaxes(pred_train_all, 0, 2)\n",
    "    obs_train = np.swapaxes(ytrain_all, 0, 1)\n",
    "    pred_val = np.swapaxes(pred_val_all, 0, 2)\n",
    "    obs_val = np.swapaxes(yval_all, 0, 1)\n",
    "\n",
    "    errors_function = [rmse, mse, mae, mape_at]\n",
    "    errors_name = ['rmse', 'mse', 'mae', 'mape_at']\n",
    "    grid_search_dict={'train':{}, 'val':{}}\n",
    "    grid_search_dict_pertimeseries = dict([(ts, {'train':{}, 'val':{}} ) for ts in time_series])\n",
    "\n",
    "    for ef,en in zip(errors_function, errors_name):\n",
    "\n",
    "        grid_search_dict['train'][en]={}\n",
    "        grid_search_dict['train'][en]['error'] = np.array([np.array([ef(np.concatenate(pred_train[ixp][ixcv]), np.concatenate(obs_train[ixcv])) \n",
    "                            for ixcv in range(pred_train.shape[1])]) for ixp in range(pred_train.shape[0])])\n",
    "\n",
    "        grid_search_dict['train'][en]['mean'] = grid_search_dict['train'][en]['error'].mean(axis=1) \n",
    "        grid_search_dict['train'][en]['std'] = grid_search_dict['train'][en]['error'].std(axis=1) \n",
    "\n",
    "        grid_search_dict['val'][en]={}\n",
    "        grid_search_dict['val'][en]['error'] = np.array([np.array([ef(np.concatenate(pred_val[ixp][ixcv]), np.concatenate(obs_val[ixcv])) \n",
    "                            for ixcv in range(pred_val.shape[1])]) for ixp in range(pred_val.shape[0])])\n",
    "\n",
    "        grid_search_dict['val'][en]['mean'] = grid_search_dict['val'][en]['error'].mean(axis=1) \n",
    "        grid_search_dict['val'][en]['std'] = grid_search_dict['val'][en]['error'].std(axis=1) \n",
    "        \n",
    "        for ix,ts in enumerate(time_series):\n",
    "            grid_search_dict_pertimeseries[ts]['train'][en] = {}\n",
    "            grid_search_dict_pertimeseries[ts]['train'][en]['error'] = np.array([np.array([ef(pred_train[ixp][ixcv][ix], obs_train[ixcv][ix]) \n",
    "                            for ixcv in range(pred_train.shape[1])]) for ixp in range(pred_train.shape[0])])\n",
    "            \n",
    "            grid_search_dict_pertimeseries[ts]['train'][en]['mean'] = grid_search_dict_pertimeseries[ts]['train'][en]['error'].mean(axis=1) \n",
    "            grid_search_dict_pertimeseries[ts]['train'][en]['std'] = grid_search_dict_pertimeseries[ts]['train'][en]['error'].std(axis=1) \n",
    "            \n",
    "            grid_search_dict_pertimeseries[ts]['val'][en] = {}\n",
    "            grid_search_dict_pertimeseries[ts]['val'][en]['error'] = np.array([np.array([ef(pred_val[ixp][ixcv][ix], obs_val[ixcv][ix]) \n",
    "                            for ixcv in range(pred_val.shape[1])]) for ixp in range(pred_val.shape[0])])\n",
    "            \n",
    "            grid_search_dict_pertimeseries[ts]['val'][en]['mean'] = grid_search_dict_pertimeseries[ts]['val'][en]['error'].mean(axis=1) \n",
    "            grid_search_dict_pertimeseries[ts]['val'][en]['std'] = grid_search_dict_pertimeseries[ts]['val'][en]['error'].std(axis=1) \n",
    "            \n",
    "\n",
    "    return grid_search_dict, grid_search_dict_pertimeseries\n",
    "\n",
    "\n",
    "def pred_list_to_dataframe(pred_list, time_series, days):\n",
    "    data = [j for i in [build_timestamp_list(d+' 00:00:00', d+ ' 23:45:00') for d in days] for j in i]\n",
    "    df = pd.DataFrame(data=data, columns=['Datetime'])\n",
    "    for ix, ts in enumerate(time_series):\n",
    "        df[ts] = pred_list[ix].reshape(pred_list[ix].shape[0]*pred_list[ix].shape[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "observation_data_path = ['/home/toque/data2/montreal/stm/data/valid_metro_15min_2015_2016_2017_sumpass_nodayfree.csv']\n",
    "exogenous_data_path = ['/home/toque/data2/montreal/events/data/clean/events_2015_2018_end_event_stopid.csv',\n",
    "                       '/home/toque/data2/montreal/events/data/clean/events_2015_2018_start_event_stopid.csv',\n",
    "                       '/home/toque/data2/montreal/events/data/clean/events_2015_2018_period_event_stopid.csv',\n",
    "                       '/home/toque/data2/weather/predicted_weather/predicted_weather_2015_2017_included_perday_pm.csv'\n",
    "                      ]\n",
    "context_data_path = ['/home/toque/data2/date/2013-01-01-2019-01-01_new.csv']\n",
    "\n",
    "df_observation = read_csv_list(observation_data_path)\n",
    "df_exogenous = read_csv_list(exogenous_data_path)\n",
    "df_context = read_csv_list(context_data_path)\n",
    "\n",
    "# fill timestamps not available with 0 to have 96 timestamps per day\n",
    "days = sorted(list(set([i[:10] for i in df_observation['Datetime'].values])))\n",
    "timestamp_list = [j for i in [build_timestamp_list(d+' 00:00:00', d+' 23:45:00', time_step_second=15*60) for d in days] for j in i]\n",
    "df_date = pd.DataFrame(data = timestamp_list, columns = ['Datetime']).set_index('Datetime')\n",
    "df_observation = df_date.join(df_observation.set_index('Datetime')).fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_series = ['11', '32', '34', '15', '44', '65', '31', '33', '35', '47', '13',\n",
    "       '14', '1', '9', '5', '18', '36', '24', '68', '43', '8', '64', '10',\n",
    "       '55', '3', '49', '51', '2', '19', '56', '7', '6', '4', '48', '66',\n",
    "       '25', '23', '28', '39', '54', '60', '27', '20', '46', '12', '21',\n",
    "       '62', '52', '41', '50', '30', '16', '37', '40', '26', '67', '57',\n",
    "       '61', '42', '45', '38', '29', '58', '63', '22', '59', '53', '17']\n",
    "\n",
    "features_exogenous = ['5-end_event', '11-end_event', '12-end_event', '13-end_event',\n",
    "       '15-end_event', '16-end_event', '23-end_event', '24-end_event',\n",
    "       '31-end_event', '32-end_event', '35-end_event', '43-end_event',\n",
    "       '45-end_event', '61-end_event', '68-end_event', '5-start_event',\n",
    "       '11-start_event', '12-start_event', '13-start_event',\n",
    "       '15-start_event', '16-start_event', '23-start_event',\n",
    "       '24-start_event', '31-start_event', '32-start_event',\n",
    "       '35-start_event', '43-start_event', '45-start_event',\n",
    "       '61-start_event', '68-start_event', '5-period_event',\n",
    "       '11-period_event', '12-period_event', '13-period_event',\n",
    "       '15-period_event', '16-period_event', '23-period_event',\n",
    "       '24-period_event', '31-period_event', '32-period_event',\n",
    "       '35-period_event', '43-period_event', '45-period_event',\n",
    "       '61-period_event', '68-period_event']\n",
    "\n",
    "\n",
    "\n",
    "features_context = ['Day_id', 'Mois_id','vac_noel_quebec', 'day_off_quebec', '24DEC', '31DEC',\n",
    "                    'renov_beaubien', 'vac_udem1', 'vac_udem2']\n",
    "\n",
    "scaler_choice_X = None\n",
    "scaler_choice_y = None\n",
    "\n",
    "param_kfold={\n",
    "    'n_splits': 5,\n",
    "    'shuffle': True,\n",
    "    'random_state': 1}\n",
    "\n",
    "param_grid={\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_features': ['auto',None],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2,5,10],\n",
    "    'min_samples_leaf': [1,5,10],\n",
    "    'n_jobs': [6],\n",
    "    'criterion': ['mse']}\n",
    "\n",
    "\n",
    "\n",
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2016-12-31 23:45:00'\n",
    "\n",
    "model_name = 'lt_rf_uni_inverted_OPTIMSERIES'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_observation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-94ddd4ca13dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_Xy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_observation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_exogenous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Datetime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_Xy_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_Xy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_datetime\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_datetime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_xy_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_Xy_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_exogenous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_observation' is not defined"
     ]
    }
   ],
   "source": [
    "df_Xy = df_observation.set_index('Datetime').join([df_context.set_index('Datetime'), df_exogenous.set_index('Datetime')])\n",
    "\n",
    "df_Xy_train = df_Xy[start_datetime:end_datetime]\n",
    "Xtrain, ytrain_list, Xnames, days = create_xy_dataset(df_Xy_train, time_series, features_exogenous, features_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization time_series: 4it [00:00, 17.93it/s]\n"
     ]
    }
   ],
   "source": [
    "grid_search_dict, grid_search_dict_pertimeseries = optimize(Xtrain, ytrain_list, param_kfold, time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_pickle('/home/toque/data2/forecast/model/rf_uni_inverted/optimize/'+model_name+'/grid_search_dict.pkl', grid_search_dict)\n",
    "save_pickle('/home/toque/data2/forecast/model/rf_uni_inverted/optimize/'+model_name+'/grid_search_dict_pertimeseries.pkl', grid_search_dict_pertimeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get best params and learn with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_dict_pertimeseries = load_pickle('/home/toque/data2/forecast/model/rf_uni_inverted/optimize/'+model_name+'/grid_search_dict_pertimeseries.pkl')\n",
    "best_arg = dict([(ts, grid_search_dict_pertimeseries[ts]['val']['rmse']['mean'].argmin()) for ts in time_series])\n",
    "keys, values = zip(*param_grid.items())\n",
    "all_params = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "best_params_pertimeseries = dict([(ts, all_params[best_arg[ts]]) for ts in time_series])\n",
    "\n",
    "\n",
    "grid_search_dict = load_pickle('/home/toque/data2/forecast/model/rf_uni_inverted/optimize/'+model_name+'/grid_search_dict.pkl')\n",
    "best_arg = grid_search_dict['val']['rmse']['mean'].argmin()\n",
    "keys, values = zip(*param_grid.items())\n",
    "all_params = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "best_params = all_params[best_arg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 5/5 [00:00<00:00, 617.01it/s]\n",
      "4it [00:00, 165.70it/s]\n"
     ]
    }
   ],
   "source": [
    "df_Xy = df_observation.set_index('Datetime').join([df_context.set_index('Datetime'), df_exogenous.set_index('Datetime')])\n",
    "df_Xy_train = df_Xy[start_datetime:end_datetime]\n",
    "Xtrain, ytrain_list, Xnames, days = create_xy_dataset(df_Xy_train, time_series, features_exogenous, features_context)\n",
    "\n",
    "rf_list = []\n",
    "for ts, ytrain in tqdm(zip(time_series, ytrain_list)):\n",
    "    best_params = best_params_pertimeseries[ts]\n",
    "    rf = RandomForestRegressor(**best_params, verbose=0)\n",
    "    rf.fit(Xtrain,ytrain)\n",
    "    rf_list.append(rf)\n",
    "    \n",
    "# Save models\n",
    "#save_pickle('/home/toque/data2/forecast/model/rf_uni_inverted/optimize/'+model_name+'/list_rf_uni_inverted.pkl', rf_list)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 1093/1093 [00:01<00:00, 616.09it/s]\n"
     ]
    }
   ],
   "source": [
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2017-12-31 23:45:00'\n",
    "df_Xy_test = df_Xy[start_datetime:end_datetime]\n",
    "Xtest, ytest_list, Xnames, days_test = create_xy_dataset(df_Xy_test, time_series, features_exogenous, features_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 571.04it/s]\n"
     ]
    }
   ],
   "source": [
    "path_directory_to_save = '/home/toque/data2/forecast/model/rf_uni_inverted/prediction/'+model_name+'/'\n",
    "pred_list = []\n",
    "for rf in tqdm(rf_list):\n",
    "    pred_list.append(rf.predict(Xtest))\n",
    "pred_list = np.array(pred_list)\n",
    "\n",
    "\n",
    "df_res = pred_list_to_dataframe(pred_list, time_series, days_test)\n",
    "\n",
    "if not os.path.exists(path_directory_to_save):\n",
    "    os.makedirs(path_directory_to_save)\n",
    "\n",
    "df_res.to_csv(path_directory_to_save + start_datetime[:10] + \"_\" + end_datetime[:10] + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>11</th>\n",
       "      <th>32</th>\n",
       "      <th>34</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>134.4</td>\n",
       "      <td>42.4</td>\n",
       "      <td>87.5</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01 00:15:00</td>\n",
       "      <td>163.2</td>\n",
       "      <td>28.6</td>\n",
       "      <td>99.8</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01 00:30:00</td>\n",
       "      <td>400.4</td>\n",
       "      <td>32.2</td>\n",
       "      <td>93.9</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01 00:45:00</td>\n",
       "      <td>179.6</td>\n",
       "      <td>17.8</td>\n",
       "      <td>45.6</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01 01:00:00</td>\n",
       "      <td>26.8</td>\n",
       "      <td>4.2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015-01-01 01:15:00</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2015-01-01 01:30:00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015-01-01 01:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-01-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2015-01-01 02:15:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015-01-01 02:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015-01-01 02:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2015-01-01 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2015-01-01 03:15:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015-01-01 03:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015-01-01 03:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2015-01-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2015-01-01 04:15:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2015-01-01 04:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015-01-01 04:45:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2015-01-01 05:00:00</td>\n",
       "      <td>15.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2015-01-01 05:15:00</td>\n",
       "      <td>25.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2015-01-01 05:30:00</td>\n",
       "      <td>50.8</td>\n",
       "      <td>9.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2015-01-01 05:45:00</td>\n",
       "      <td>47.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.2</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2015-01-01 06:00:00</td>\n",
       "      <td>49.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2015-01-01 06:15:00</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2015-01-01 06:30:00</td>\n",
       "      <td>39.6</td>\n",
       "      <td>5.2</td>\n",
       "      <td>27.5</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2015-01-01 06:45:00</td>\n",
       "      <td>47.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2015-01-01 07:00:00</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>22.5</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2015-01-01 07:15:00</td>\n",
       "      <td>50.6</td>\n",
       "      <td>13.0</td>\n",
       "      <td>41.2</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104898</th>\n",
       "      <td>2017-12-31 16:30:00</td>\n",
       "      <td>377.4</td>\n",
       "      <td>578.7</td>\n",
       "      <td>235.9</td>\n",
       "      <td>126.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104899</th>\n",
       "      <td>2017-12-31 16:45:00</td>\n",
       "      <td>465.0</td>\n",
       "      <td>594.5</td>\n",
       "      <td>226.7</td>\n",
       "      <td>171.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104900</th>\n",
       "      <td>2017-12-31 17:00:00</td>\n",
       "      <td>508.8</td>\n",
       "      <td>893.0</td>\n",
       "      <td>223.8</td>\n",
       "      <td>225.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104901</th>\n",
       "      <td>2017-12-31 17:15:00</td>\n",
       "      <td>467.6</td>\n",
       "      <td>665.9</td>\n",
       "      <td>234.5</td>\n",
       "      <td>214.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104902</th>\n",
       "      <td>2017-12-31 17:30:00</td>\n",
       "      <td>384.6</td>\n",
       "      <td>538.7</td>\n",
       "      <td>206.6</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104903</th>\n",
       "      <td>2017-12-31 17:45:00</td>\n",
       "      <td>449.4</td>\n",
       "      <td>483.5</td>\n",
       "      <td>207.5</td>\n",
       "      <td>171.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104904</th>\n",
       "      <td>2017-12-31 18:00:00</td>\n",
       "      <td>426.6</td>\n",
       "      <td>451.9</td>\n",
       "      <td>196.0</td>\n",
       "      <td>209.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104905</th>\n",
       "      <td>2017-12-31 18:15:00</td>\n",
       "      <td>320.2</td>\n",
       "      <td>319.6</td>\n",
       "      <td>191.5</td>\n",
       "      <td>188.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104906</th>\n",
       "      <td>2017-12-31 18:30:00</td>\n",
       "      <td>258.8</td>\n",
       "      <td>293.3</td>\n",
       "      <td>171.4</td>\n",
       "      <td>128.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104907</th>\n",
       "      <td>2017-12-31 18:45:00</td>\n",
       "      <td>264.6</td>\n",
       "      <td>240.1</td>\n",
       "      <td>160.2</td>\n",
       "      <td>124.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104908</th>\n",
       "      <td>2017-12-31 19:00:00</td>\n",
       "      <td>226.6</td>\n",
       "      <td>220.2</td>\n",
       "      <td>155.2</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104909</th>\n",
       "      <td>2017-12-31 19:15:00</td>\n",
       "      <td>199.2</td>\n",
       "      <td>201.2</td>\n",
       "      <td>173.0</td>\n",
       "      <td>96.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104910</th>\n",
       "      <td>2017-12-31 19:30:00</td>\n",
       "      <td>180.8</td>\n",
       "      <td>172.5</td>\n",
       "      <td>149.9</td>\n",
       "      <td>94.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104911</th>\n",
       "      <td>2017-12-31 19:45:00</td>\n",
       "      <td>199.8</td>\n",
       "      <td>146.7</td>\n",
       "      <td>150.1</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104912</th>\n",
       "      <td>2017-12-31 20:00:00</td>\n",
       "      <td>187.8</td>\n",
       "      <td>147.0</td>\n",
       "      <td>149.3</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104913</th>\n",
       "      <td>2017-12-31 20:15:00</td>\n",
       "      <td>220.2</td>\n",
       "      <td>126.9</td>\n",
       "      <td>144.0</td>\n",
       "      <td>102.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104914</th>\n",
       "      <td>2017-12-31 20:30:00</td>\n",
       "      <td>208.8</td>\n",
       "      <td>107.3</td>\n",
       "      <td>152.2</td>\n",
       "      <td>105.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104915</th>\n",
       "      <td>2017-12-31 20:45:00</td>\n",
       "      <td>203.8</td>\n",
       "      <td>86.2</td>\n",
       "      <td>133.4</td>\n",
       "      <td>113.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104916</th>\n",
       "      <td>2017-12-31 21:00:00</td>\n",
       "      <td>160.2</td>\n",
       "      <td>126.0</td>\n",
       "      <td>136.5</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104917</th>\n",
       "      <td>2017-12-31 21:15:00</td>\n",
       "      <td>199.0</td>\n",
       "      <td>101.1</td>\n",
       "      <td>146.3</td>\n",
       "      <td>117.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104918</th>\n",
       "      <td>2017-12-31 21:30:00</td>\n",
       "      <td>245.4</td>\n",
       "      <td>121.4</td>\n",
       "      <td>110.8</td>\n",
       "      <td>63.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104919</th>\n",
       "      <td>2017-12-31 21:45:00</td>\n",
       "      <td>220.8</td>\n",
       "      <td>104.5</td>\n",
       "      <td>130.6</td>\n",
       "      <td>66.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104920</th>\n",
       "      <td>2017-12-31 22:00:00</td>\n",
       "      <td>238.0</td>\n",
       "      <td>64.9</td>\n",
       "      <td>148.8</td>\n",
       "      <td>105.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104921</th>\n",
       "      <td>2017-12-31 22:15:00</td>\n",
       "      <td>208.6</td>\n",
       "      <td>52.8</td>\n",
       "      <td>134.0</td>\n",
       "      <td>86.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104922</th>\n",
       "      <td>2017-12-31 22:30:00</td>\n",
       "      <td>169.6</td>\n",
       "      <td>46.3</td>\n",
       "      <td>138.5</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104923</th>\n",
       "      <td>2017-12-31 22:45:00</td>\n",
       "      <td>160.4</td>\n",
       "      <td>34.1</td>\n",
       "      <td>113.6</td>\n",
       "      <td>44.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104924</th>\n",
       "      <td>2017-12-31 23:00:00</td>\n",
       "      <td>195.4</td>\n",
       "      <td>46.8</td>\n",
       "      <td>130.2</td>\n",
       "      <td>51.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104925</th>\n",
       "      <td>2017-12-31 23:15:00</td>\n",
       "      <td>142.0</td>\n",
       "      <td>30.7</td>\n",
       "      <td>104.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104926</th>\n",
       "      <td>2017-12-31 23:30:00</td>\n",
       "      <td>151.6</td>\n",
       "      <td>32.6</td>\n",
       "      <td>109.3</td>\n",
       "      <td>64.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104927</th>\n",
       "      <td>2017-12-31 23:45:00</td>\n",
       "      <td>140.4</td>\n",
       "      <td>32.3</td>\n",
       "      <td>89.2</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104928 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime     11     32     34     15\n",
       "0       2015-01-01 00:00:00  134.4   42.4   87.5   62.0\n",
       "1       2015-01-01 00:15:00  163.2   28.6   99.8   36.0\n",
       "2       2015-01-01 00:30:00  400.4   32.2   93.9   42.0\n",
       "3       2015-01-01 00:45:00  179.6   17.8   45.6   17.0\n",
       "4       2015-01-01 01:00:00   26.8    4.2   14.5   16.0\n",
       "5       2015-01-01 01:15:00    1.8    0.2    0.0    3.0\n",
       "6       2015-01-01 01:30:00    1.2    0.0    0.0    0.0\n",
       "7       2015-01-01 01:45:00    0.0    0.0    0.0    0.0\n",
       "8       2015-01-01 02:00:00    0.0    0.0    0.0    0.0\n",
       "9       2015-01-01 02:15:00    0.0    0.0    0.0    0.0\n",
       "10      2015-01-01 02:30:00    0.0    0.0    0.0    0.0\n",
       "11      2015-01-01 02:45:00    0.0    0.0    0.0    0.0\n",
       "12      2015-01-01 03:00:00    0.0    0.0    0.0    0.0\n",
       "13      2015-01-01 03:15:00    0.0    0.0    0.0    0.0\n",
       "14      2015-01-01 03:30:00    0.0    0.0    0.0    0.0\n",
       "15      2015-01-01 03:45:00    0.0    0.0    0.0    0.0\n",
       "16      2015-01-01 04:00:00    0.0    0.0    0.0    0.0\n",
       "17      2015-01-01 04:15:00    0.0    0.0    0.0    0.0\n",
       "18      2015-01-01 04:30:00    0.0    0.0    0.0    0.0\n",
       "19      2015-01-01 04:45:00    0.0    0.0    0.0    0.0\n",
       "20      2015-01-01 05:00:00   15.4    0.8    5.3    1.0\n",
       "21      2015-01-01 05:15:00   25.8    6.4    6.7    3.0\n",
       "22      2015-01-01 05:30:00   50.8    9.0   33.6   12.0\n",
       "23      2015-01-01 05:45:00   47.8    6.0   27.2   20.0\n",
       "24      2015-01-01 06:00:00   49.8    8.6   25.0   19.0\n",
       "25      2015-01-01 06:15:00   51.0    3.6   27.0   21.0\n",
       "26      2015-01-01 06:30:00   39.6    5.2   27.5   23.0\n",
       "27      2015-01-01 06:45:00   47.8    4.0   20.1   21.0\n",
       "28      2015-01-01 07:00:00   52.0    5.2   22.5   15.0\n",
       "29      2015-01-01 07:15:00   50.6   13.0   41.2   24.0\n",
       "...                     ...    ...    ...    ...    ...\n",
       "104898  2017-12-31 16:30:00  377.4  578.7  235.9  126.6\n",
       "104899  2017-12-31 16:45:00  465.0  594.5  226.7  171.4\n",
       "104900  2017-12-31 17:00:00  508.8  893.0  223.8  225.8\n",
       "104901  2017-12-31 17:15:00  467.6  665.9  234.5  214.6\n",
       "104902  2017-12-31 17:30:00  384.6  538.7  206.6  196.0\n",
       "104903  2017-12-31 17:45:00  449.4  483.5  207.5  171.4\n",
       "104904  2017-12-31 18:00:00  426.6  451.9  196.0  209.2\n",
       "104905  2017-12-31 18:15:00  320.2  319.6  191.5  188.2\n",
       "104906  2017-12-31 18:30:00  258.8  293.3  171.4  128.2\n",
       "104907  2017-12-31 18:45:00  264.6  240.1  160.2  124.6\n",
       "104908  2017-12-31 19:00:00  226.6  220.2  155.2  150.0\n",
       "104909  2017-12-31 19:15:00  199.2  201.2  173.0   96.8\n",
       "104910  2017-12-31 19:30:00  180.8  172.5  149.9   94.2\n",
       "104911  2017-12-31 19:45:00  199.8  146.7  150.1   74.0\n",
       "104912  2017-12-31 20:00:00  187.8  147.0  149.3   95.0\n",
       "104913  2017-12-31 20:15:00  220.2  126.9  144.0  102.4\n",
       "104914  2017-12-31 20:30:00  208.8  107.3  152.2  105.8\n",
       "104915  2017-12-31 20:45:00  203.8   86.2  133.4  113.2\n",
       "104916  2017-12-31 21:00:00  160.2  126.0  136.5  122.0\n",
       "104917  2017-12-31 21:15:00  199.0  101.1  146.3  117.8\n",
       "104918  2017-12-31 21:30:00  245.4  121.4  110.8   63.8\n",
       "104919  2017-12-31 21:45:00  220.8  104.5  130.6   66.4\n",
       "104920  2017-12-31 22:00:00  238.0   64.9  148.8  105.4\n",
       "104921  2017-12-31 22:15:00  208.6   52.8  134.0   86.2\n",
       "104922  2017-12-31 22:30:00  169.6   46.3  138.5   57.0\n",
       "104923  2017-12-31 22:45:00  160.4   34.1  113.6   44.6\n",
       "104924  2017-12-31 23:00:00  195.4   46.8  130.2   51.8\n",
       "104925  2017-12-31 23:15:00  142.0   30.7  104.0   50.0\n",
       "104926  2017-12-31 23:30:00  151.6   32.6  109.3   64.8\n",
       "104927  2017-12-31 23:45:00  140.4   32.3   89.2   44.0\n",
       "\n",
       "[104928 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [02:08<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "p= {'criterion': 'mse',\n",
    " 'max_depth': None,\n",
    " 'max_features': 'auto',\n",
    " 'min_samples_leaf': 1,\n",
    " 'min_samples_split': 10,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': 6}\n",
    "rf_list_15stations = []\n",
    "for ytrain in tqdm(ytrain_list):\n",
    "    rf = RandomForestRegressor(**p, verbose=0)\n",
    "    rf.fit(Xtrain,ytrain)\n",
    "    rf_list_15stations.append(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 1093/1093 [00:09<00:00, 118.02it/s]\n",
      "100%|██████████| 68/68 [00:14<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6100480166\n",
      "29.609964402\n",
      "876.749991889\n",
      "12.6160008791\n"
     ]
    }
   ],
   "source": [
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2017-12-31 23:45:00'\n",
    "df_Xy_pred = df_Xy[start_datetime:end_datetime]\n",
    "Xpred, ypred_list, Xnames, days_pred = create_xy_dataset(df_Xy_pred, time_series, features_exogenous, features_context)\n",
    "\n",
    "pred_list = []\n",
    "for rf in tqdm(rf_list_15stations):\n",
    "    pred_list.append(rf.predict(Xpred))\n",
    "pred_list = np.array(pred_list)\n",
    "\n",
    "print(mape_at(ypred_list, pred_list))\n",
    "print(rmse(ypred_list, pred_list))\n",
    "print(mse(ypred_list, pred_list))\n",
    "print(mae(ypred_list, pred_list))\n",
    "\n",
    "df = pred_list_to_dataframe(pred_list, time_series, days_pred)\n",
    "\n",
    "df.to_csv('/home/toque/data2/forecast/model/rf_inverted/prediction/rf_inverted_15stationsexo_contextcal_withoutoptim6min/2015-01-01_2016-12-31.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 731/731 [00:06<00:00, 116.31it/s]\n",
      "100%|██████████| 68/68 [01:42<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "features_exogenous = [ '12-start_event',\n",
    "  '13-start_event', '15-start_event', '16-start_event',\n",
    "  '23-start_event', '24-start_event', '31-start_event',\n",
    "  '32-start_event', '45-start_event', '61-start_event', '12-end_event', '13-end_event', '15-end_event',\n",
    "  '16-end_event', '23-end_event', '24-end_event', '31-end_event', '32-end_event', '45-end_event',\n",
    "  '61-end_event', '12-period_event', '13-period_event', '15-period_event',\n",
    "  '16-period_event', '23-period_event', '24-period_event',\n",
    "  '31-period_event', '32-period_event', '45-period_event', '61-period_event']\n",
    "\n",
    "\n",
    "df_Xy = df_observation.set_index('Datetime').join([df_context.set_index('Datetime'), df_exogenous.set_index('Datetime')])\n",
    "\n",
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2016-12-31 23:45:00'\n",
    "df_Xy_train = df_Xy[start_datetime:end_datetime]\n",
    "Xtrain, ytrain_list, Xnames, days = create_xy_dataset(df_Xy_train, time_series, features_exogenous, features_context)\n",
    "\n",
    "rf_list_10stations = []\n",
    "for ytrain in tqdm(ytrain_list):\n",
    "    rf = RandomForestRegressor(**p, verbose=0)\n",
    "    rf.fit(Xtrain,ytrain)\n",
    "    rf_list_10stations.append(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 1093/1093 [00:09<00:00, 117.12it/s]\n"
     ]
    }
   ],
   "source": [
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2017-12-31 23:45:00'\n",
    "df_Xy_pred = df_Xy[start_datetime:end_datetime]\n",
    "Xpred, ypred_list, Xnames, days_pred = create_xy_dataset(df_Xy_pred, time_series, features_exogenous, features_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:14<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6349150232\n",
      "29.6660906757\n",
      "880.076935981\n",
      "12.6231035849\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "for rf in tqdm(rf_list_10stations):\n",
    "    pred_list.append(rf.predict(Xpred))\n",
    "pred_list = np.array(pred_list)\n",
    "\n",
    "print(mape_at(ypred_list, pred_list))\n",
    "print(rmse(ypred_list, pred_list))\n",
    "print(mse(ypred_list, pred_list))\n",
    "print(mae(ypred_list, pred_list))\n",
    "\n",
    "df = pred_list_to_dataframe(pred_list, time_series, days_pred)\n",
    "\n",
    "df.to_csv('/home/toque/data2/forecast/model/rf_inverted/prediction/rf_inverted_10stationsexo_contextcal_withoutoptim6min/2015-01-01_2016-12-31.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 731/731 [00:06<00:00, 115.23it/s]\n",
      "100%|██████████| 68/68 [00:22<00:00,  2.98it/s]\n"
     ]
    }
   ],
   "source": [
    "features_exogenous = []\n",
    "\n",
    "\n",
    "df_Xy = df_observation.set_index('Datetime').join([df_context.set_index('Datetime'), df_exogenous.set_index('Datetime')])\n",
    "\n",
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2016-12-31 23:45:00'\n",
    "df_Xy_train = df_Xy[start_datetime:end_datetime]\n",
    "Xtrain, ytrain_list, Xnames, days = create_xy_dataset(df_Xy_train, time_series, features_exogenous, features_context)\n",
    "\n",
    "rf_list_0stations = []\n",
    "for ytrain in tqdm(ytrain_list):\n",
    "    rf = RandomForestRegressor(**p, verbose=0)\n",
    "    rf.fit(Xtrain,ytrain)\n",
    "    rf_list_0stations.append(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 1093/1093 [00:09<00:00, 115.44it/s]\n"
     ]
    }
   ],
   "source": [
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2017-12-31 23:45:00'\n",
    "df_Xy_pred = df_Xy[start_datetime:end_datetime]\n",
    "Xpred, ypred_list, Xnames, days_pred = create_xy_dataset(df_Xy_pred, time_series, features_exogenous, features_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:13<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0824746327\n",
      "34.2750358755\n",
      "1174.77808426\n",
      "13.6830421636\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "for rf in tqdm(rf_list_0stations):\n",
    "    pred_list.append(rf.predict(Xpred))\n",
    "pred_list = np.array(pred_list)\n",
    "\n",
    "print(mape_at(ypred_list, pred_list))\n",
    "print(rmse(ypred_list, pred_list))\n",
    "print(mse(ypred_list, pred_list))\n",
    "print(mae(ypred_list, pred_list))\n",
    "\n",
    "df = pred_list_to_dataframe(pred_list, time_series, days_pred)\n",
    "\n",
    "df.to_csv('/home/toque/data2/forecast/model/rf_inverted/prediction/rf_inverted_0stationsexo_contextcal_withoutoptim6min/2015-01-01_2016-12-31.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meteo day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 731/731 [00:06<00:00, 106.66it/s]\n",
      "100%|██████████| 68/68 [00:31<00:00,  2.14it/s]\n"
     ]
    }
   ],
   "source": [
    "features_context = ['Day_id', 'Mois_id','vac_noel_quebec', 'day_off_quebec', '24DEC', '31DEC',\n",
    "                    'renov_beaubien', 'vac_udem1', 'vac_udem2', 'Temperature_min_celcius', 'Temperature_max_celcius',\n",
    "                    'Humidex_celcius', 'Windchill_celcius', 'Probability', 'Water_height_mm', 'Snow_height_cm']\n",
    "features_exogenous = []\n",
    "\n",
    "df_Xy = df_observation.set_index('Datetime').join([df_context.set_index('Datetime'), df_exogenous.set_index('Datetime')])\n",
    "\n",
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2016-12-31 23:45:00'\n",
    "df_Xy_train = df_Xy[start_datetime:end_datetime]\n",
    "Xtrain, ytrain_list, Xnames, days = create_xy_dataset(df_Xy_train, time_series, features_exogenous, features_context)\n",
    "\n",
    "rf_list_meteo = []\n",
    "for ytrain in tqdm(ytrain_list):\n",
    "    rf = RandomForestRegressor(**p, verbose=0)\n",
    "    rf.fit(Xtrain,ytrain)\n",
    "    rf_list_meteo.append(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 1027/1027 [00:09<00:00, 104.33it/s]\n"
     ]
    }
   ],
   "source": [
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2017-12-31 23:45:00'\n",
    "df_Xy_pred = df_Xy[start_datetime:end_datetime]\n",
    "Xpred, ypred_list, Xnames, days_pred = create_xy_dataset(df_Xy_pred, time_series, features_exogenous, features_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:14<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.2560931804\n",
      "31.0098156654\n",
      "961.608667603\n",
      "12.7690460991\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "for rf in tqdm(rf_list_meteo):\n",
    "    pred_list.append(rf.predict(Xpred))\n",
    "pred_list = np.array(pred_list)\n",
    "\n",
    "print(mape_at(ypred_list, pred_list))\n",
    "print(rmse(ypred_list, pred_list))\n",
    "print(mse(ypred_list, pred_list))\n",
    "print(mae(ypred_list, pred_list))\n",
    "\n",
    "df = pred_list_to_dataframe(pred_list, time_series, days_pred)\n",
    "\n",
    "df.to_csv('/home/toque/data2/forecast/model/rf_inverted/prediction/rf_inverted_0stationsexo_contextcalmeteoday_withoutoptim6min/2015-01-01_2016-12-31.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meteo day + event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 731/731 [00:06<00:00, 105.55it/s]\n",
      "100%|██████████| 68/68 [02:17<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "features_context = ['Day_id', 'Mois_id','vac_noel_quebec', 'day_off_quebec', '24DEC', '31DEC',\n",
    "                    'renov_beaubien', 'vac_udem1', 'vac_udem2', 'Temperature_min_celcius', 'Temperature_max_celcius',\n",
    "                    'Humidex_celcius', 'Windchill_celcius', 'Probability', 'Water_height_mm', 'Snow_height_cm']\n",
    "features_exogenous = ['5-end_event', '11-end_event', '12-end_event', '13-end_event',\n",
    "       '15-end_event', '16-end_event', '23-end_event', '24-end_event',\n",
    "       '31-end_event', '32-end_event', '35-end_event', '43-end_event',\n",
    "       '45-end_event', '61-end_event', '68-end_event', '5-start_event',\n",
    "       '11-start_event', '12-start_event', '13-start_event',\n",
    "       '15-start_event', '16-start_event', '23-start_event',\n",
    "       '24-start_event', '31-start_event', '32-start_event',\n",
    "       '35-start_event', '43-start_event', '45-start_event',\n",
    "       '61-start_event', '68-start_event', '5-period_event',\n",
    "       '11-period_event', '12-period_event', '13-period_event',\n",
    "       '15-period_event', '16-period_event', '23-period_event',\n",
    "       '24-period_event', '31-period_event', '32-period_event',\n",
    "       '35-period_event', '43-period_event', '45-period_event',\n",
    "       '61-period_event', '68-period_event']\n",
    "\n",
    "df_Xy = df_observation.set_index('Datetime').join([df_context.set_index('Datetime'), df_exogenous.set_index('Datetime')])\n",
    "\n",
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2016-12-31 23:45:00'\n",
    "df_Xy_train = df_Xy[start_datetime:end_datetime]\n",
    "Xtrain, ytrain_list, Xnames, days = create_xy_dataset(df_Xy_train, time_series, features_exogenous, features_context)\n",
    "\n",
    "rf_list_meteo15stations = []\n",
    "for ytrain in tqdm(ytrain_list):\n",
    "    rf = RandomForestRegressor(**p, verbose=0)\n",
    "    rf.fit(Xtrain,ytrain)\n",
    "    rf_list_meteo15stations.append(rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Days loop: 100%|██████████| 1027/1027 [00:09<00:00, 107.15it/s]\n"
     ]
    }
   ],
   "source": [
    "start_datetime, end_datetime = '2015-01-01 00:00:00', '2017-12-31 23:45:00'\n",
    "df_Xy_pred = df_Xy[start_datetime:end_datetime]\n",
    "Xpred, ypred_list, Xnames, days_pred = create_xy_dataset(df_Xy_pred, time_series, features_exogenous, features_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:14<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.2203841311\n",
      "28.0771396727\n",
      "788.3257722\n",
      "12.1563958957\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "for rf in tqdm(rf_list_meteo15stations):\n",
    "    pred_list.append(rf.predict(Xpred))\n",
    "pred_list = np.array(pred_list)\n",
    "\n",
    "print(mape_at(ypred_list, pred_list))\n",
    "print(rmse(ypred_list, pred_list))\n",
    "print(mse(ypred_list, pred_list))\n",
    "print(mae(ypred_list, pred_list))\n",
    "\n",
    "df = pred_list_to_dataframe(pred_list, time_series, days_pred)\n",
    "\n",
    "df.to_csv('/home/toque/data2/forecast/model/rf_inverted/prediction/rf_inverted_15stationsexo_contextcalmeteoday_withoutoptim6min/2015-01-01_2016-12-31.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn with best params/features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction -> csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_list_to_dataframe(pred_list, time_series, days):\n",
    "    data = [j for i in [build_timestamp_list(d+' 00:00:00', d+ ' 23:45:00') for d in days] for j in i]\n",
    "    df = pd.DataFrame(data=data, columns=['Datetime'])\n",
    "    for ix, ts in enumerate(time_series):\n",
    "        df[ts] = pred_list[ix].reshape(pred_list[ix].shape[0]*pred_list[ix].shape[1])\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 - env",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
