{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wget\n",
    "import time\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from  tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '../utils')\n",
    "from utils_date import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_file(url, filename_path):\n",
    "    \n",
    "    \n",
    "    directory = os.path.dirname(filename_path)\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    filename = wget.download(url,filename_path)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5.6147401332855225 seconds ---\n",
      "--- 4.040607690811157 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 721/721 [00:05<00:00, 134.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value incomplete 0.0008%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    Filter on stations of Metro line 1\\n'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def main(path_to_save, full_path_date):\n",
    "    \"\"\"\n",
    "        Load data from open data paris API and remove them \n",
    "    \"\"\"\n",
    "\n",
    "    url_data = 'https://opendata.stif.info/explore/dataset/histo-validations/files/ff00c4d0d1ac37b823f5ba854a895d8e/download/'\n",
    "    filename_data = download_file(url_data, path_to_save+'data_2017S1.zip')\n",
    "    zip_ref = zipfile.ZipFile(filename_data, 'r')\n",
    "    zip_ref.extractall(os.path.dirname(filename_data)+'/')\n",
    "    zip_ref.close()\n",
    "\n",
    "    os.remove(filename_data)\n",
    "    os.remove(path_to_save+'DATA PUBLIEES S1 2017/2017S1_NB_SURFACE.txt')\n",
    "    os.remove(path_to_save+'DATA PUBLIEES S1 2017/2017S1_PROFIL_SURFACE.txt')\n",
    "\n",
    "    filename_profil = path_to_save+'DATA PUBLIEES S1 2017/2017S1_PROFIL_FER.txt'\n",
    "    filename_validations = path_to_save+'DATA PUBLIEES S1 2017/2017S1_NB_FER.txt'\n",
    "\n",
    "    df_validations = pd.read_csv(filename_validations, sep='\\t')\n",
    "    df_profil = pd.read_csv(filename_profil, sep='\\t')\n",
    "\n",
    "    shutil.rmtree(path_to_save+'DATA PUBLIEES S1 2017/', ignore_errors=True)\n",
    "\n",
    "\n",
    "    url_data = 'https://opendata.stif.info/explore/dataset/emplacement-des-gares-idf/download/?format=csv&timezone=America/New_York&use_labels_for_header=true'\n",
    "    filename_data = download_file(url_data,path_to_save+'station_information.csv')\n",
    "\n",
    "    df_station = pd.read_csv(filename_data, sep=';')\n",
    "\n",
    "    os.remove(filename_data)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        Clean data\n",
    "    \"\"\"\n",
    "\n",
    "    dict_idrl_csa = dict(df_validations[['ID_REFA_LDA', 'CODE_STIF_ARRET']].drop_duplicates().values)\n",
    "    dict_csa_idrl = dict(df_validations[['CODE_STIF_ARRET', 'ID_REFA_LDA' ]].drop_duplicates().values)\n",
    "\n",
    "    # Modif df_validations\n",
    "\n",
    "    df_validations = df_validations[['JOUR','ID_REFA_LDA','NB_VALD']]\n",
    "    df_validations = df_validations.dropna()\n",
    "    df_validations['NB_VALD'] = np.array([1 if i[0]=='M' else i for i in df_validations['NB_VALD'].values ]).astype(int)\n",
    "    df_validations = df_validations[df_validations.ID_REFA_LDA != '?']\n",
    "    df_validations.ID_REFA_LDA = df_validations.ID_REFA_LDA.values.astype(int).astype(str)\n",
    "    df_validations['JOUR'] = [i.split('/')[2]+'-'+i.split('/')[1]+'-'+i.split('/')[0] for i in df_validations['JOUR']]\n",
    "\n",
    "    # Modif df_profil\n",
    "    df_profil = df_profil[['TRNC_HORR_60', 'ID_REFA_LDA','CAT_JOUR', 'pourc_validations']]\n",
    "    df_profil = df_profil.dropna()\n",
    "    df_profil = df_profil[df_profil.TRNC_HORR_60 != 'ND']\n",
    "    df_profil = df_profil[df_profil.ID_REFA_LDA != '?']\n",
    "    df_profil.ID_REFA_LDA = df_profil.ID_REFA_LDA.values.astype(int).astype(str)\n",
    "    df_profil['TRNC_HORR_60'] = df_profil['TRNC_HORR_60'].values.astype(str)\n",
    "    df_profil = df_profil.drop_duplicates()\n",
    "\n",
    "    # Modif df_station\n",
    "    df_station['GARES_ID'] = df_station['GARES_ID'].values.astype(str)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        Create dictionaries\n",
    "    \"\"\"\n",
    "\n",
    "    # 1 - dict_idstation_typeofday_percentage with df_profil\n",
    "    trnc_horr_60 = ['0H-1H', '1H-2H', '2H-3H', '3H-4H', '4H-5H', '5H-6H', '6H-7H', '7H-8H', '8H-9H', '9H-10H',\n",
    "                    '10H-11H', '11H-12H', '12H-13H', '13H-14H', '14H-15H', '15H-16H', '16H-17H', '17H-18H',\n",
    "                    '18H-19H', '19H-20H', '20H-21H', '21H-22H', '22H-23H', '23H-0H']\n",
    "\n",
    "    time_60 = [i[11:] for i in build_timestamp_list(\"2000-01-01 00:00:00\",\"2000-01-01 23:00:00\",time_step_second = 60*60)]\n",
    "\n",
    "    df = pd.pivot_table( df_profil, values = ['pourc_validations'],\n",
    "                        index = ['ID_REFA_LDA','CAT_JOUR'],\n",
    "                        columns = 'TRNC_HORR_60',fill_value=0)\n",
    "\n",
    "    df.columns = df.columns.droplevel(0)\n",
    "    df.columns.name = None\n",
    "    df = df.reset_index()\n",
    "    df = df[['ID_REFA_LDA', 'CAT_JOUR'] + trnc_horr_60]\n",
    "\n",
    "    list_index_incomplete = df['ID_REFA_LDA'].drop_duplicates().values[df[['ID_REFA_LDA']].groupby('ID_REFA_LDA').size().values!=5]\n",
    "\n",
    "    for i in list_index_incomplete:\n",
    "        df = df[df['ID_REFA_LDA'] !=i ]\n",
    "\n",
    "    dict_idstation_typeofday_percentage = dict([((i[0],i[1]),np.array(i[2:]).astype(float)) for i in df.values])\n",
    "\n",
    "    # 2 - dict_idstation_day_sumvalidation with df_profil\n",
    "\n",
    "    df = df_validations.groupby(['JOUR','ID_REFA_LDA']).sum().reset_index()\n",
    "    df = df[['ID_REFA_LDA', 'JOUR', 'NB_VALD']]\n",
    "    dict_idstation_day_sumvalidation = dict([((i[0],i[1]),i[2])for i in df.values])\n",
    "\n",
    "    # 3 - dict_day_category1\n",
    "    df_exogenous = pd.read_csv(full_path_date)\n",
    "    dict_day_category1 = dict([(i[:10],j) for i,j in df_exogenous[['Datetime','Category1']].values])\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        Create and reshape dataframe datetime_values\n",
    "    \"\"\"\n",
    "\n",
    "    id_stations_validations = np.array(sorted(set([i[0] for i in dict_idstation_day_sumvalidation.keys()])))\n",
    "    id_stations_profil = np.array(sorted(set([i[0] for i in dict_idstation_typeofday_percentage.keys()])))\n",
    "    id_stations = list(filter(lambda x: x in id_stations_validations, id_stations_profil))\n",
    "    day_list = sorted(df_validations['JOUR'].drop_duplicates().tolist())\n",
    "\n",
    "\n",
    "    dict_trnc_time = dict(zip(trnc_horr_60, time_60) )\n",
    "\n",
    "    values = [build_timestamp_list(day+\" 00:00:00\", day+\" 23:00:00\", time_step_second=60*60)\n",
    "              for day in day_list]\n",
    "    values =  [item for sublist in values for item in sublist]\n",
    "    df_datetime_values = pd.DataFrame(data = values, columns= ['Datetime'])\n",
    "\n",
    "    columns = ['Date']+trnc_horr_60\n",
    "    list_df=[]\n",
    "    value_incomplete=0\n",
    "\n",
    "    for ids in tqdm(id_stations):\n",
    "        data = []\n",
    "        for day in day_list:\n",
    "            typeofday = dict_day_category1[day]\n",
    "            try:\n",
    "                v = dict_idstation_day_sumvalidation[(ids,day)] * dict_idstation_typeofday_percentage[(ids,typeofday)]\n",
    "            except:\n",
    "                value_incomplete=+1\n",
    "                v = np.zeros(24)\n",
    "            data.append([day]+list(v))\n",
    "\n",
    "        df = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "        df = df.set_index(\"Date\").stack().reset_index()\n",
    "        df.columns = ['Date', 'trnc_horr_60', ids]\n",
    "        df['time'] = [dict_trnc_time[i] for i in df['trnc_horr_60'].values]\n",
    "        df['Datetime'] = [i + \" \" + j for i,j in df[['Date','time']].values]\n",
    "        list_df.append(df[['Datetime', ids]].set_index('Datetime'))\n",
    "\n",
    "    print(\"Value incomplete {:.4f}%\".format((value_incomplete/(len(id_stations)*len(day_list)*1.))*100))\n",
    "\n",
    "    df_datetime_values = df_datetime_values.set_index('Datetime').join(list_df).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "        Filter on stations of Metro line 1 and 7 for the test\n",
    "    \"\"\"\n",
    "\n",
    "    df_station_ = df_station[['MODE_', 'LIGNE', 'GARES_ID']].groupby(['MODE_','LIGNE'])['GARES_ID'].apply(lambda x: \"%s\" % ','.join(x)).reset_index()\n",
    "    dict_modeligne_csa = dict([((i[0],i[1]), i[2].split(',')) for i in df_station_[['MODE_', 'LIGNE', 'GARES_ID']].values])\n",
    "\n",
    "    filter_modeligne = [('Metro','1'),('Metro','7')]\n",
    "    filter_station_id = []\n",
    "    for i in filter_modeligne:\n",
    "\n",
    "            for j in dict_modeligne_csa[i]:\n",
    "\n",
    "                try:\n",
    "                    filter_station_id.append(dict_csa_idrl[j])\n",
    "                except:\n",
    "                    print(\"Fail to link station with CodeStifArret {} with IdRefaLda\".format(j))\n",
    "\n",
    "    filter_station_id_ = []\n",
    "    for i in filter_station_id:\n",
    "        if i in id_stations:\n",
    "            filter_station_id_.append(i)\n",
    "        else:\n",
    "            print(\"Fail to link station id {} with datetime_values file\".format(i))\n",
    "\n",
    "\n",
    "    df_datetime_values_filtered = df_datetime_values[ ['Datetime'] + filter_station_id_]\n",
    "    df_datetime_values_filtered.to_csv(path_to_save+'observation_file_2017-01-01_2017-06-30_included_test.csv', index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-p\", \"--path_to_save\", default=\"/home/toque/data2/forecast/test/\")\n",
    "    parser.add_argument(\"-fpd\", \"--full_path_date\", default=\"/home/toque/data2/forecast/test/\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    path_to_save = args.path_to_save\n",
    "    full_path_date = args.full_path_date\n",
    "    main(path_to_save, full_path_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.5500779151916504 seconds ---\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_station_ = df_station[['MODE_', 'LIGNE', 'GARES_ID']].groupby(['MODE_','LIGNE'])['GARES_ID'].apply(lambda x: \"%s\" % ','.join(x)).reset_index()\n",
    "dict_modeligne_csa = dict([((i[0],i[1]), i[2].split(',')) for i in df_station_[['MODE_', 'LIGNE', 'GARES_ID']].values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to link station with CodeStifArret 962 with IdRefaLda\n",
      "Fail to link station with CodeStifArret 166 with IdRefaLda\n",
      "Fail to link station with CodeStifArret 169 with IdRefaLda\n",
      "Fail to link station with CodeStifArret 654 with IdRefaLda\n",
      "Fail to link station with CodeStifArret 715 with IdRefaLda\n",
      "Fail to link station with CodeStifArret 619 with IdRefaLda\n",
      "Fail to link station with CodeStifArret 315 with IdRefaLda\n",
      "Fail to link station with CodeStifArret 384 with IdRefaLda\n",
      "Fail to link station with CodeStifArret 810 with IdRefaLda\n",
      "Fail to link station with CodeStifArret 607 with IdRefaLda\n"
     ]
    }
   ],
   "source": [
    "filter_modeligne = [('Metro','1'),('Metro','7')]\n",
    "filter_station_id = []\n",
    "for i in filter_modeligne:\n",
    "    \n",
    "        for j in dict_modeligne_csa[i]:\n",
    "     \n",
    "            try:\n",
    "                filter_station_id.append(dict_csa_idrl[j])\n",
    "            except:\n",
    "                print(\"Fail to link station with CodeStifArret {} with IdRefaLda\".format(j))\n",
    "                \n",
    "filter_station_id_ = []\n",
    "for i in filter_station_id:\n",
    "    if i in id_stations:\n",
    "        filter_station_id_.append(i)\n",
    "    else:\n",
    "        print(\"Fail to link station id {} with datetime_values file\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datetime_values_filtered = df_datetime_values[ ['Datetime'] + filter_station_id_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datetime_values_filtered.to_csv('../data/observation_file_2017-01-01_2017-06-30_included_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 - env",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
